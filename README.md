# Image Colorization using Conditional Generative Adversarial Networks

## Introduction

Colorization of images is a challenging task due to the diverse imaging situations that a single algorithm must handle. This project employs Conditional Generative Adversarial Networks (CGANs) to colorize black and white images, showcasing the real-world application of machine learning. Unlike deterministic methods, our model predicts plausible colors for grayscale images, making it feasible to visualize historical images in color. The project addresses the ill-posed nature of the problem, considering the variability in colors for both manmade and natural objects.

## Literature Review

Several deep learning-based colorization techniques have been proposed. Papers such as "Colorful Image Colorization," "Let there be Color!" and "Image-to-Image Translation with Conditional Adversarial Networks (pix2pix)" offer insights into different approaches. Our model adopts the pix2pix approach, incorporating L1 and GAN losses.

## Dataset

The COCO unlabeled images 2017 dataset, containing around 123,000 images, was chosen for its royalty-free availability. Approximately 10,000 images were used for training due to computational constraints. The L*a*b color space was employed, simplifying the prediction of color channels compared to RGB.

## Baseline

The baseline model, utilizing ResNet18 as the backbone, produced colorized outputs without training.

## Main Approach

Our model is a CGAN with an additional L1 loss. The generator, based on the UNet architecture with ResNet-18 backbone, predicts color channels (*a, *b) from the input grayscale image (L channel). The discriminator assesses the realism of the generated image. Pretraining the generator using L1 loss is followed by adversarial training.

## Evaluation Metric

Subjectivity in assessing image realism makes it challenging to define concrete evaluation metrics. A Google Form gathered peer opinions on the realism of images generated by our model.

## Results & Analysis

The model showed impressive results, particularly in images featuring grass, trees, and skies. Limited computational resources allowed training for 10 epochs (pretraining) and 16 epochs (adversarial training), taking approximately 6 hours. Further training could significantly improve results.

## Future Work

Given more resources, the model would be trained for at least 100 epochs with a larger dataset. A graphical user interface for user interaction and support for variable-sized input images are potential enhancements.

## Results
![Results](https://imageupload.io/ib/F9Hd601kuCLi1xd_1697873228.jpg)
